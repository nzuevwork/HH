# Postmortem: OOMKilled → CrashLoopBackOff приложения

**Дата инцидента:** 05.02.2026
**Длительность:** ~21 минута
**Затронутый сервис:** веб-приложение (основной ingress endpoint)
**Серьёзность:** SEV-2 (частичная недоступность)

---

## Impact (влияние на пользователя)

Сервис начал возвращать HTTP 5xx ответы.
Часть пользователей не могла открыть сайт.

* до 40–60% запросов завершались ошибкой
* страницы загружались с большой задержкой
* наблюдались периодические таймауты

Полного падения кластера не было, но сервис фактически считался недоступным.

---

## Detection (как была обнаружена проблема)

Проблема была обнаружена через мониторинг:

* всплеск HTTP 5xx в метриках ingress
* рост перезапусков pod’а в Grafana
* alert по доступности сервиса

После проверки кластера:

```
kubectl get pods
```

обнаружен статус:

```
CrashLoopBackOff
```

---

## Timeline

**21:14** — сработал alert High 5xx rate
**21:15** — проверен ingress (подозрение на routing)
**21:17** — обнаружен pod в состоянии CrashLoopBackOff
**21:18** — просмотр событий pod
**21:20** — анализ логов контейнера
**21:23** — обнаружено завершение процесса
**21:24** — `kubectl describe pod` показал OOMKilled
**21:26** — проверены limits памяти Deployment
**21:28** — увеличен memory limit
**21:30** — rolling restart Deployment
**21:35** — сервис стабилизирован

---

## Root Cause (первопричина)

Контейнер завершался из-за нехватки памяти.

После обновления приложения фактическое потребление RAM увеличилось,
но в Kubernetes остались старые memory limits.

В результате kubelet завершал контейнер по OOMKilled,
после чего pod попадал в CrashLoopBackOff.

---

## Resolution (как была восстановлена работа)

Выполнены действия:

1. проверка логов контейнера
2. анализ событий Kubernetes
3. выявление OOMKilled
4. увеличение `resources.limits.memory`
5. перезапуск Deployment (rolling restart)

После обновления pod перезапуски прекратились, HTTP 5xx исчезли.

---

## Corrective Actions (предотвращение повторения)

Чтобы избежать повторения инцидента:

* увеличены memory limits
* добавлен alert на использование памяти pod
* добавлен мониторинг рестартов контейнеров
* внедрена проверка ресурсов перед деплоем

---

## Lessons Learned

* обновление приложения может менять потребление ресурсов
* отсутствие мониторинга памяти приводит к позднему обнаружению
* CrashLoopBackOff — симптом, а не причина
* метрики ingress позволяют быстрее обнаружить деградацию

---

## Итог

Сервис восстановлен.
Повторных OOMKilled после изменения limits не наблюдается.

Инцидент подтвердил необходимость мониторинга ресурсов pod’ов и проверки конфигурации перед деплоем.

